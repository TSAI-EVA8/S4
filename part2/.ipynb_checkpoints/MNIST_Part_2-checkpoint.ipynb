{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbWi8mPah39u"
   },
   "source": [
    "## The assignment is about classifying the MNIST digit images correctly \n",
    "There are few rules:\n",
    "\n",
    "1. Number of trainable parameters to be <20K\n",
    "2. The validation accuracy >=99.4%\n",
    "3. Number of epochs to reach the desired accuracy <20\n",
    "4. Most important no use of FC layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0m2JWFliFfKT"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZqMuXpm-W_xr",
    "outputId": "c224b8c5-3435-4412-cd08-8e666588917d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "h_Cx9q2QFgM7"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # input - 28x28x1; output - 22x22x32;\n",
    "        self.conv1 = nn.Sequential(\n",
    "            #RF=3x3\n",
    "            nn.Conv2d(1, 16, 3,bias=False),#28x28x1 > 26x26x16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            #RF=5x5\n",
    "            nn.Conv2d(16, 16, 3,bias=False), #26x26x16> op=24*24*16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "       \n",
    "            #RF=7x7\n",
    "            nn.Conv2d(16, 32, 3,bias=False), #24*24*16 > op=22*22*32\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.1),\n",
    "        )\n",
    "\n",
    "        ##transalation layer..this is doing the following\n",
    "        ##1. doubling the RF\n",
    "        ##2. reducing the channels by half\n",
    "        ##3. redcuing the size of the image by half\n",
    "        ##input->22x22x32 .. output->11x11x16\n",
    "        self.trans1 = nn.Sequential(\n",
    "            # RF - 7x7\n",
    "            nn.Conv2d(32, 16, 1,bias=False), #22x22x32 >22x22x16\n",
    "            nn.ReLU(),\n",
    "\n",
    "            ##maxpooling RF->14x14\n",
    "            nn.MaxPool2d(2,2), #22x22x16 > 11x11x16\n",
    "        )\n",
    "\n",
    "        \n",
    "        ##input 11x11x16  output:7x7x16\n",
    "        self.conv2 = nn.Sequential(\n",
    "            #RF=16x16\n",
    "            nn.Conv2d(16, 16, 3,bias=False),#11x11x16>9x9x16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            #RF=18x18\n",
    "            nn.Conv2d(16, 16, 3,bias=False), #9x9x16>op=7*7*16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "        )\n",
    "\n",
    "        # input - 7x7x16; output - 5x5x16\n",
    "        self.conv3 = nn.Sequential(\n",
    "            # RF - 20x20\n",
    "            nn.Conv2d(16, 16, 3, padding=1, bias=False), # 7x7x16>7x7x16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "\n",
    "            # RF - 22x22\n",
    "            nn.Conv2d(16, 16, 3, bias=False), #7x7x16> 5x5x16\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.1),\n",
    "        )\n",
    "\n",
    "        \n",
    "        # GAP Layer ..this will be used to make the final output\n",
    "        # input - 5x5x16; output - 1x1x10\n",
    "        self.avg_pool = nn.Sequential(\n",
    "            # # RF - 24x24\n",
    "            nn.Conv2d(16, 10, 1, bias=False),#5x5x16>5x5x10\n",
    "            nn.AvgPool2d(5)#5x5x10>1x1x10\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.trans1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdydjYTZFyi3",
    "outputId": "f519111d-210e-4b68-a006-200a23396bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 26, 26]             144\n",
      "              ReLU-2           [-1, 16, 26, 26]               0\n",
      "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
      "         Dropout2d-4           [-1, 16, 26, 26]               0\n",
      "            Conv2d-5           [-1, 16, 24, 24]           2,304\n",
      "              ReLU-6           [-1, 16, 24, 24]               0\n",
      "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
      "         Dropout2d-8           [-1, 16, 24, 24]               0\n",
      "            Conv2d-9           [-1, 32, 22, 22]           4,608\n",
      "             ReLU-10           [-1, 32, 22, 22]               0\n",
      "      BatchNorm2d-11           [-1, 32, 22, 22]              64\n",
      "        Dropout2d-12           [-1, 32, 22, 22]               0\n",
      "           Conv2d-13           [-1, 16, 22, 22]             512\n",
      "             ReLU-14           [-1, 16, 22, 22]               0\n",
      "        MaxPool2d-15           [-1, 16, 11, 11]               0\n",
      "           Conv2d-16             [-1, 16, 9, 9]           2,304\n",
      "             ReLU-17             [-1, 16, 9, 9]               0\n",
      "      BatchNorm2d-18             [-1, 16, 9, 9]              32\n",
      "        Dropout2d-19             [-1, 16, 9, 9]               0\n",
      "           Conv2d-20             [-1, 16, 7, 7]           2,304\n",
      "             ReLU-21             [-1, 16, 7, 7]               0\n",
      "      BatchNorm2d-22             [-1, 16, 7, 7]              32\n",
      "        Dropout2d-23             [-1, 16, 7, 7]               0\n",
      "           Conv2d-24             [-1, 16, 7, 7]           2,304\n",
      "             ReLU-25             [-1, 16, 7, 7]               0\n",
      "      BatchNorm2d-26             [-1, 16, 7, 7]              32\n",
      "        Dropout2d-27             [-1, 16, 7, 7]               0\n",
      "           Conv2d-28             [-1, 16, 5, 5]           2,304\n",
      "             ReLU-29             [-1, 16, 5, 5]               0\n",
      "      BatchNorm2d-30             [-1, 16, 5, 5]              32\n",
      "        Dropout2d-31             [-1, 16, 5, 5]               0\n",
      "           Conv2d-32             [-1, 10, 5, 5]             160\n",
      "        AvgPool2d-33             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 17,200\n",
      "Trainable params: 17,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.32\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 1.39\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-31c0df4d5117>:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "## number of paramters are < 20K which satisfies the assignment condition1\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)\n",
    "summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DqTWLaM5GHgH"
   },
   "outputs": [],
   "source": [
    "\n",
    "## read the MNIST data set \n",
    "torch.manual_seed(1)\n",
    "batch_size = 128\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "HLuYgmR7-X6M",
    "outputId": "b73e40be-fceb-4d46-850f-2c271df9cfac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fda0a6996a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAM7klEQVR4nO3db4hd9Z3H8c9H2zwwLZhs0jBabbpFxbCwdgmxsqKW2vrnSaxCSZDFUmGqNFBhcTdWocqmIN11FxEsTKk0ka6lMEqlrG1tKKb7IMVRNCaTJs5KpBnHjG4e1OqDRP32wT0po849Z3LPOffc5Pt+wXDvPd/75+vRj+fc87vn/BwRAnD6O6PrBgAMB2EHkiDsQBKEHUiCsANJfGyYH2abQ/9AyyLCiy2vtWW3fa3tA7ZnbG+t814A2uVBx9ltnynpoKQvSzos6VlJmyNiuuQ1bNmBlrWxZd8gaSYiXomIY5J+KmljjfcD0KI6YT9X0h8XPD5cLPsA2+O2p2xP1fgsADW1foAuIiYkTUjsxgNdqrNln5V03oLHny6WARhBdcL+rKQLbH/W9jJJmyQ92UxbAJo28G58RLxre4ukX0k6U9IjEbGvsc4ANGrgobeBPozv7EDrWvlRDYBTB2EHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSQx1ymbgZDz88MOl9R07dpTWd+/e3WQ7pzy27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsGFnr168vre/fv7+0zjj7B9UKu+1Dkt6S9J6kdyOi/N8OgM40sWX/YkS82cD7AGgR39mBJOqGPST92vZztscXe4LtcdtTtqdqfhaAGuruxl8eEbO2PyXpadt/iIhdC58QEROSJiTJdtT8PAADqrVlj4jZ4nZe0hOSNjTRFIDmDRx228ttf/LEfUlfkbS3qcYANKvObvwaSU/YPvE+/x0Rv2ykK0DV4+yPPvrokDo5PQwc9oh4RdLfN9gLgBYx9AYkQdiBJAg7kARhB5Ig7EASnOKKzlx55ZW1Xv/MM8801EkObNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2UfAunXrSuv33XdfaX3btm19ay+++OJAPQ3DWWedVev1Vettz549td7/dMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9BNxzzz2l9Ztuuqm0vmvXrr61UR5nr2v16tVdt3BKYcsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzj4CqqYmLqbF7uudd95psp1TxlNPPdV1C6eUyi277Udsz9veu2DZSttP2365uF3RbpsA6lrKbvyPJV37oWVbJe2MiAsk7SweAxhhlWGPiF2Sjn5o8UZJ24v72yXd0HBfABo26Hf2NRExV9x/XdKafk+0PS5pfMDPAdCQ2gfoIiJsR0l9QtKEJJU9D0C7Bh16O2J7TJKK2/nmWgLQhkHD/qSkW4r7t0j6eTPtAGhL5W687cckXSVple3Dkr4r6X5JP7N9q6RXJX2tzSZPdWvXri2tn3/++aX1Q4cOldZ37Nhxkh2NhqrrvqNZlWGPiM19Sl9quBcALeLnskAShB1IgrADSRB2IAnCDiTBKa5DcNlll5XWly1bVlqfmZkprR8/fvykexoFVUOOVf/cVXV8EFt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYhuPHGG0vrEeUX8Nm2bVuT7QzV8uXL+9aqLqH9xhtvNN1OamzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtkbUDaWLElXXHFFab1qSubZ2dnSetn58MeOHSt9bdvGxsb61qrO83/ooYeabic1tuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7A247bbbSuurVq0qrVedz37w4MHSetmUzrt37y597eTkZK16HVX/3GhW5Zbd9iO2523vXbDsXtuztl8o/q5vt00AdS1lN/7Hkq5dZPl/RcQlxd//NNsWgKZVhj0idkk6OoReALSozgG6Lbb3FLv5K/o9yfa47SnbUzU+C0BNg4b9B5I+J+kSSXOSHuj3xIiYiIj1EVF+dUEArRoo7BFxJCLei4j3Jf1Q0oZm2wLQtIHCbnvheYtflbS333MBjIbKcXbbj0m6StIq24clfVfSVbYvkRSSDkn6Zos9jryqecarzle/6667SutV87dffPHFfWtXX3116Ws3bdpUWq+yb9++0voZZ/TfnlStl7m5udL6OeecU1p/7bXXSuvZVIY9IjYvsvhHLfQCoEX8XBZIgrADSRB2IAnCDiRB2IEkPMzTDG2fluc0Pvjgg6X1LVu2lNYvuuii0vrMzMxJ93TC2WefXVq/++67B35vSVq9enVp/brrrutbqzr1t2po7u233y6tX3rppX1r09PTpa89lUXEoiuOLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4ewNuv/320vo111xTWr/55ptL61XjyaNsfn6+b63sEtiStGED10QZBOPsQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+yoZd26daX1vXv7TynwwAN9JxKSJN15550D9ZQd4+xAcoQdSIKwA0kQdiAJwg4kQdiBJAg7kETlLK5AmbLrwleZnJxssBNUqdyy2z7P9m9tT9veZ/vbxfKVtp+2/XJxu6L9dgEMaim78e9K+ueIWCfpC5K+ZXudpK2SdkbEBZJ2Fo8BjKjKsEfEXEQ8X9x/S9J+SedK2ihpe/G07ZJuaKtJAPWd1Hd222slfV7S7yWtiYi5ovS6pDV9XjMuaXzwFgE0YclH421/QtKkpDsi4k8La9E7m2bRk1wiYiIi1kfE+lqdAqhlSWG3/XH1gv6TiHi8WHzE9lhRH5PU/zKiADpXeYqre/Pmbpd0NCLuWLD83yX9f0Tcb3urpJUR8S8V78UprqeZAwcOlNaXLVvWt3bhhReWvvb48eMD9ZRdv1Ncl/Kd/R8l/ZOkl2y/UCz7jqT7Jf3M9q2SXpX0tSYaBdCOyrBHxP9KWvT/FJK+1Gw7ANrCz2WBJAg7kARhB5Ig7EAShB1IglNcUUvvZxj9zczM9K0xjj5cbNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2VFL1fUQpqenh9QJqrBlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkKq8b3+iHcd14oHX9rhvPlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkqgMu+3zbP/W9rTtfba/XSy/1/as7ReKv+vbbxfAoCp/VGN7TNJYRDxv+5OSnpN0g3rzsf85Iv5jyR/Gj2qA1vX7Uc1S5mefkzRX3H/L9n5J5zbbHoC2ndR3dttrJX1e0u+LRVts77H9iO0VfV4zbnvK9lStTgHUsuTfxtv+hKRnJH0vIh63vUbSm5JC0r+pt6v/jYr3YDceaFm/3fglhd32xyX9QtKvIuI/F6mvlfSLiPi7ivch7EDLBj4Rxr1pOn8kaf/CoBcH7k74qqS9dZsE0J6lHI2/XNLvJL0k6f1i8XckbZZ0iXq78YckfbM4mFf2XmzZgZbV2o1vCmEH2sf57EByhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQqLzjZsDclvbrg8api2Sga1d5GtS+J3gbVZG+f6VcY6vnsH/lweyoi1nfWQIlR7W1U+5LobVDD6o3deCAJwg4k0XXYJzr+/DKj2tuo9iXR26CG0lun39kBDE/XW3YAQ0LYgSQ6Cbvta20fsD1je2sXPfRj+5Dtl4ppqDudn66YQ2/e9t4Fy1baftr2y8XtonPsddTbSEzjXTLNeKfrruvpz4f+nd32mZIOSvqypMOSnpW0OSKmh9pIH7YPSVofEZ3/AMP2FZL+LGnHiam1bH9f0tGIuL/4H+WKiPjXEentXp3kNN4t9dZvmvGvq8N11+T054PoYsu+QdJMRLwSEcck/VTSxg76GHkRsUvS0Q8t3ihpe3F/u3r/sQxdn95GQkTMRcTzxf23JJ2YZrzTdVfS11B0EfZzJf1xwePDGq353kPSr20/Z3u862YWsWbBNFuvS1rTZTOLqJzGe5g+NM34yKy7QaY/r4sDdB91eUT8g6TrJH2r2F0dSdH7DjZKY6c/kPQ59eYAnJP0QJfNFNOMT0q6IyL+tLDW5bpbpK+hrLcuwj4r6bwFjz9dLBsJETFb3M5LekK9rx2j5MiJGXSL2/mO+/mriDgSEe9FxPuSfqgO110xzfikpJ9ExOPF4s7X3WJ9DWu9dRH2ZyVdYPuztpdJ2iTpyQ76+Ajby4sDJ7K9XNJXNHpTUT8p6Zbi/i2Sft5hLx8wKtN495tmXB2vu86nP4+Iof9Jul69I/L/J+nuLnro09ffSnqx+NvXdW+SHlNvt+64esc2bpX0N5J2SnpZ0m8krRyh3h5Vb2rvPeoFa6yj3i5Xbxd9j6QXir/ru153JX0NZb3xc1kgCQ7QAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EASfwE71Qd2f0T+JwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Code to view the MNIST digits\n",
    "import matplotlib.pyplot as plt\n",
    "samples=next(iter(train_loader))\n",
    "images,labels=samples\n",
    "images.shape\n",
    "print(labels[0])\n",
    "plt.imshow(images[0].data.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8fDefDhaFlwH"
   },
   "outputs": [],
   "source": [
    "## this is the trainign code which is same as the Rohan's colab. Only added some comments\n",
    "from tqdm import tqdm\n",
    "\n",
    "## trainign function\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    ## sets the model in the training mode\n",
    "    model.train()\n",
    "    ## this will show the progress bar as the model is trained\n",
    "    pbar = tqdm(train_loader)\n",
    "    ## we will train the model in mini batches by getting the data from the data loader\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        #pushing the data and the label to the cuda\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        ## zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        ## forward pass\n",
    "        output = model(data)\n",
    "        ## calculate the NLL loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        ## perform back prop\n",
    "        loss.backward()\n",
    "        ## update the weights\n",
    "        optimizer.step()\n",
    "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
    "\n",
    "\n",
    "## the test function\n",
    "def test(model, device, test_loader):\n",
    "    ##set the model in evaluation mode\n",
    "    model.eval()\n",
    "    ## set the loss to zero\n",
    "    test_loss = 0\n",
    "    ## the number of correct classifications\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        ## get the test batch from the test loader\n",
    "        for data, target in test_loader:\n",
    "            ## mode the data to the cuda\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            ## forward pass\n",
    "            output = model(data)\n",
    "            ## calculate the loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            ## get the index of the max log-prob\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            ## add the number of the correct outputs in this batch\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    ## the total test loss is the loss divided by the total number of items\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMWbLWO6FuHb",
    "outputId": "ea26238a-c737-41b2-cb5e-2bda4a95404d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-26-31c0df4d5117>:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "loss=0.23272907733917236 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0740, Accuracy: 9785/10000 (97.85%)\n",
      "\n",
      "\n",
      "Epoch 2 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.07478306442499161 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9886/10000 (98.86%)\n",
      "\n",
      "\n",
      "Epoch 3 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.06217704340815544 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0335, Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "\n",
      "Epoch 4 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03401017189025879 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0293, Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "\n",
      "Epoch 5 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.025988832116127014 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9916/10000 (99.16%)\n",
      "\n",
      "\n",
      "Epoch 6 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03670281916856766 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0280, Accuracy: 9916/10000 (99.16%)\n",
      "\n",
      "\n",
      "Epoch 7 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.021878859028220177 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0263, Accuracy: 9921/10000 (99.21%)\n",
      "\n",
      "\n",
      "Epoch 8 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03182286396622658 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0214, Accuracy: 9935/10000 (99.35%)\n",
      "\n",
      "\n",
      "Epoch 9 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.05876306816935539 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0227, Accuracy: 9932/10000 (99.32%)\n",
      "\n",
      "\n",
      "Epoch 10 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.09428905695676804 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0249, Accuracy: 9930/10000 (99.30%)\n",
      "\n",
      "\n",
      "Epoch 11 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.09558454900979996 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0223, Accuracy: 9933/10000 (99.33%)\n",
      "\n",
      "\n",
      "Epoch 12 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.07145363092422485 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0205, Accuracy: 9938/10000 (99.38%)\n",
      "\n",
      "\n",
      "Epoch 13 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.0698014497756958 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0235, Accuracy: 9920/10000 (99.20%)\n",
      "\n",
      "\n",
      "Epoch 14 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.04358208552002907 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0212, Accuracy: 9937/10000 (99.37%)\n",
      "\n",
      "\n",
      "Epoch 15 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.02668958343565464 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0192, Accuracy: 9935/10000 (99.35%)\n",
      "\n",
      "\n",
      "Epoch 16 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03768238052725792 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0210, Accuracy: 9943/10000 (99.43%)\n",
      "\n",
      "\n",
      "Epoch 17 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.02877570502460003 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0214, Accuracy: 9936/10000 (99.36%)\n",
      "\n",
      "\n",
      "Epoch 18 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03959706425666809 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0194, Accuracy: 9936/10000 (99.36%)\n",
      "\n",
      "\n",
      "Epoch 19 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.03941606730222702 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 29.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0180, Accuracy: 9947/10000 (99.47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    print('\\nEpoch {} : '.format(epoch))\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "So5uk4EkHW6R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uegpq5kNkewj"
   },
   "source": [
    "We can see that the model hit the accuracy of 99.4 at epoch 16 and also at epoch 19"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
